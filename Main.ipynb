{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5G Localization",
   "id": "aa51527d8d132495"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read matlab files to dataframes",
   "id": "d7a0621d83968b2"
  },
  {
   "cell_type": "code",
   "id": "a503af1d-b56c-4cc9-bb77-88d8a63bdbd5",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.data_loader import load_matlab_file_as_df\n",
    "from scripts.weighted_coverage import get_miss_ref_value\n",
    "\n",
    "# source file\n",
    "BASE_DIR = \"data/\"\n",
    "FULL_DATA_SET = \"Campaign_data_NBIoT_1_2_3_4_5_6_interpolated_smoothed.mat\"\n",
    "filename = os.path.join(BASE_DIR, FULL_DATA_SET)\n",
    "\n",
    "# load the dataset as pandas dataframe\n",
    "df_smooth = load_matlab_file_as_df(\n",
    "    filename=filename,\n",
    "    dataset='dataSet_smooth',  # dataSet, dataSet_interp or dataSet_smooth\n",
    "    usecols=['lat', 'lng', 'measurements_matrix']\n",
    ")\n",
    "\n",
    "df = load_matlab_file_as_df(\n",
    "    filename=filename,\n",
    "    dataset='dataSet',  # dataSet, dataSet_interp or dataSet_smooth\n",
    "    usecols=['lat', 'lng', 'measurements_matrix']\n",
    ")\n",
    "# better printing of dataframes\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # No limit on column width\n",
    "pd.set_option('display.width', 1000)  # Set the display width to 1000 characters\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7845fa9c531dc2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare data",
   "id": "7e05d2d4d84da3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Flatten the nested measurements_matrix\n",
    "flattened_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    lat = row['lat']\n",
    "    lng = row['lng']\n",
    "    measurements_matrix = row['measurements_matrix']\n",
    "    for _, measurement in measurements_matrix.iterrows():\n",
    "        flattened_row = {'lat': lat, 'lng': lng}\n",
    "        flattened_row.update(measurement.to_dict())\n",
    "        flattened_data.append(flattened_row)\n",
    "\n",
    "# Create a new DataFrame from the flattened data\n",
    "flattened_df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Drop invalid rows\n",
    "flattened_df.dropna(inplace=True)\n",
    "\n",
    "print(flattened_df)"
   ],
   "id": "2111567c42e3de8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model",
   "id": "752d0fd6f55d0a06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f4a5556d734e1e41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scripts.haversine import haversine_distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "print(f'Training KNN regressor with {flattened_df.shape[0]} samples')\n",
    "\n",
    "# Features: Include RSSI and other relevant features\n",
    "X = flattened_df[['RSSI', 'NPCI', 'eNodeBID', 'NSINR', 'NRSRP', 'NRSRQ', 'ToA', 'operatorID', 'campaignID']]\n",
    "\n",
    "# Target: Latitude and Longitude\n",
    "Y = flattened_df[['lat', 'lng']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the KNN regressor\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate Haversine distances for each pair of true and predicted coordinates\n",
    "distances = np.array(\n",
    "    [haversine_distance(y_test.iloc[i, 0], y_test.iloc[i, 1], y_pred[i, 0], y_pred[i, 1]) for i in\n",
    "     range(len(y_test))])\n",
    "\n",
    "distances_km = distances[:, 0]\n",
    "\n",
    "# Calculate the Mean Squared Error of the Haversine distances\n",
    "mse_haversine = np.mean(distances_km ** 2)\n",
    "\n",
    "print(f'Mean Squared Error (Haversine Distance): {mse_haversine}')"
   ],
   "id": "33f0bc896e86b6a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Weighted Coverage strategy\n",
   "id": "372733b5717c593a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "44ca4870494e96b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Main script\n",
    "# Assuming flattened_df is your DataFrame containing the data\n",
    "tp_probability = 0.3\n",
    "flattened_df['point_type'] = np.random.rand(flattened_df.shape[0]) <= tp_probability\n",
    "\n",
    "rps = flattened_df[flattened_df['point_type'] == False].reset_index(drop=True)\n",
    "tps = flattened_df[flattened_df['point_type'] == True].reset_index(drop=True)\n",
    "\n",
    "operators = [1, 10]\n",
    "rf_param = 'NSINR'\n",
    "miss_ref_value = get_miss_ref_value(rf_param)\n",
    "\n",
    "# Concatenate, drop duplicates, and reset index\n",
    "unique_npcis = (pd.concat([rps[['NPCI', 'operatorID']], tps[['NPCI', 'operatorID']]])\n",
    "                .drop_duplicates()\n",
    "                .reset_index(drop=True)\n",
    "                .astype({'NPCI': 'int', 'operatorID': 'int'}))\n",
    "\n",
    "# Filter the unique NPCIs to only include the selected operators\n",
    "unique_npcis = unique_npcis[unique_npcis['operatorID'].isin(operators)]\n",
    "\n",
    "# Print the value counts of NPCIs and the resulting unique_npcis DataFrame\n",
    "print(unique_npcis['NPCI'].value_counts())\n",
    "print(unique_npcis)\n",
    "\n",
    "# m_rfp = create_point_matrix(rps, unique_npcis, rf_param)\n",
    "# m_tp = create_point_matrix(tps, unique_npcis, rf_param)\n",
    "# \n",
    "# # Calculate distances\n",
    "# d = cdist(m_tp, m_rfp, 'euclidean')\n",
    "# \n",
    "# # Adjust distances for common NPCIs\n",
    "# idx1 = (m_tp != miss_ref_value).astype(int)\n",
    "# idx2 = (m_rfp != miss_ref_value).astype(int)\n",
    "# tps_located = np.zeros(tps.shape[0], dtype=bool)\n",
    "# \n",
    "# for i in range(tps.shape[0]):\n",
    "#     s = np.sum(np.logical_and(idx1[i, :], idx2), axis=1)\n",
    "#     tps_located[i] = np.any(s > 0)\n",
    "#     d[i, s == 0] = np.inf\n",
    "#     if np.any(s != 0):\n",
    "#         d[i, s != 0] /= s[s != 0]\n",
    "# \n",
    "# # Handle zero distances\n",
    "# d[d == 0] = np.min(d[d != 0]) / 20\n",
    "# \n",
    "# print(d[0])"
   ],
   "id": "c6a8d4f6c611cef5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3e1cb0fc81a7624c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sort distances and calculate weights\n",
    "d_sort = np.sort(d, axis=1)\n",
    "idx_sort = np.argsort(d, axis=1)\n",
    "w = 1.0 / d_sort\n",
    "\n",
    "print(d_sort)\n"
   ],
   "id": "96749f3dda03c04e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Coordinates of RPs\n",
    "rfp_coordinates = rps[['lat', 'lng']].values\n",
    "\n",
    "# Estimate TP locations\n",
    "k_max = 5  # Example value, adjust as needed\n",
    "tp_est_location = []\n",
    "\n",
    "for this_k in range(1, k_max + 1):\n",
    "    rfp_selected_idx = idx_sort[tps_located, :this_k]\n",
    "    lat_k_rfp_matrix = rfp_coordinates[rfp_selected_idx][:, :, 0]\n",
    "    long_k_rfp_matrix = rfp_coordinates[rfp_selected_idx][:, :, 1]\n",
    "\n",
    "    sum_lat = np.sum(lat_k_rfp_matrix * w[tps_located, :this_k], axis=1)\n",
    "    sum_long = np.sum(long_k_rfp_matrix * w[tps_located, :this_k], axis=1)\n",
    "\n",
    "    lat_k_tp = sum_lat / np.sum(w[tps_located, :this_k], axis=1)\n",
    "    long_k_tp = sum_long / np.sum(w[tps_located, :this_k], axis=1)\n",
    "\n",
    "    tp_est_location_k = np.full((tps.shape[0], 2), np.nan)\n",
    "    tp_est_location_k[tps_located, 0] = lat_k_tp\n",
    "    tp_est_location_k[tps_located, 1] = long_k_tp\n",
    "\n",
    "    tp_est_location.append(tp_est_location_k)\n"
   ],
   "id": "f35be2b356c3c352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract original positions of the TPs\n",
    "original_positions = tps[['lat', 'lng']].values\n",
    "\n",
    "# Initialize arrays to store the estimated positions\n",
    "estimated_positions = np.full_like(original_positions, np.nan)\n",
    "\n",
    "# Fill the estimated positions array with the estimated locations\n",
    "for i, est_pos in enumerate(tp_est_location[-1]):\n",
    "    if tps_located[i]:\n",
    "        estimated_positions[i] = est_pos\n",
    "\n",
    "distances = np.array(\n",
    "    [haversine_distance(original_positions[i, 0], original_positions[i, 1], estimated_positions[i, 0],\n",
    "                        estimated_positions[i, 1]) for\n",
    "     i in\n",
    "     range(len(original_positions))])\n",
    "\n",
    "distances_km = distances[:, 0]\n",
    "\n",
    "distances_km = distances_km[~np.isnan(distances_km)]\n",
    "\n",
    "# Calculate the Mean Squared Error of the Haversine distances\n",
    "mse_haversine = np.mean(distances_km ** 2.0)\n",
    "\n",
    "print(f'Mean Squared Error (Haversine Distance): {mse_haversine}')\n"
   ],
   "id": "31cd9cc51baee8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "35e2d7d69bce6d54",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
